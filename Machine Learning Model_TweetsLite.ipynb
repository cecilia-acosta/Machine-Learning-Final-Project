{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk import word_tokenize, WordPunctTokenizer, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Keras\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Plotting \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASS Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "\n",
    "TASS-2017: Workshop on Semantic Analysis at SEPLN (http://www.sepln.org/workshops/tass/2017/)\n",
    "\n",
    "TASS Workshop on Semantic Analysis has been held since 2012, under the umbrella of the International Conference of the Spanish Society for Natural Language Processing (SEPLN). TASS was the first shared task on sentiment analysis in Twitter in Spanish. The initial aim of TASS was the furtherance of research on sentiment analysis in Spanish with a special interest on the language used in Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('Resources/general-train-tagged-3l.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'tweetText':[],'polarity_value':[]})\n",
    "row=0\n",
    "for tweet in root:\n",
    "    tweetText = tweet.find('content').text\n",
    "    lang = tweet.find('lang').text\n",
    "    polarity_value = tweet.find('sentiments').find('polarity').find('value').text\n",
    "\n",
    "    if lang == 'es':\n",
    "        train_data.loc[row] = [tweetText,polarity_value]\n",
    "        row+=1\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.set_index(\"polarity_value\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(\"NONE\", axis=0)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reset_index()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['polarity_value'] = train_data['polarity_value'].replace(['NEU'],0)\n",
    "train_data['polarity_value'] = train_data['polarity_value'].replace(['P'],1)\n",
    "train_data['polarity_value'] = train_data['polarity_value'].replace(['N'],-1)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[['tweetText','polarity_value']]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.polarity_value.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['processed_tweet'] = train_data.tweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMLO Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 days before/after the video scandal from Pio Lopez Obrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_tweets = pd.read_csv('Resources/Tweets.csv')\n",
    "amlo_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_tweets=amlo_tweets.rename(columns={'Content':'tweetText'})\n",
    "amlo_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_tweets = amlo_tweets[[\"tweetText\",\"Date\"]]\n",
    "amlo_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_tweets['processed_tweet'] = amlo_tweets.tweetText\n",
    "amlo_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "hash_regex = re.compile(r\"#(\\w+)\")\n",
    "hashtags = [] \n",
    "def hash_repl(match):\n",
    "    _ = '__HASH__'+match.group(1).upper()\n",
    "    hashtags.append(_)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = re.compile(r\"(http|https|ftp)://[a-zA-Z0-9\\./]+\")\n",
    "def url_repl(match):\n",
    "    return '__URL__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE);\n",
    "def rpt_repl(match):\n",
    "    return match.group(1)+match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "re.sub(rpt_regex, rpt_repl, \"Reppppppeated characters in wordsssss\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_regex = re.compile(r\"@(\\w+)\")\n",
    "usr_names = [] # To store the user names so we can exclude them from some parts of the analysis\n",
    "def user_repl(match):\n",
    "    _ = '__USER__'+match.group(1).upper()\n",
    "    usr_names.append(_)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting by word boundaries\n",
    "word_bound_regex = re.compile(r\"\\W+\")\n",
    "\n",
    "# Punctuations\n",
    "punctuations = \\\n",
    "    [   \n",
    "        ('__PEXCL__',    ['!', '¡', ] )    ,\\\n",
    "        ('__PQUES__',    ['?', '¿', ] )    ,\\\n",
    "        ('__PPROG__',    ['...', '…', ] )  ,\\\n",
    "    ]\n",
    "\n",
    "#For punctuation replacement\n",
    "def punctuations_repl(match):\n",
    "    text = match.group(0)\n",
    "    repl = []\n",
    "    for (key, parr) in punctuations :\n",
    "        for punc in parr :\n",
    "            if punc in text:\n",
    "                repl.append(key)\n",
    "    if(len(repl)>0 ) :\n",
    "        return ' '+' '.join(repl)+' '\n",
    "    else :\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#stop_words=stopwords.words('spanish')\n",
    "\n",
    "#def stopwords(text):\n",
    "#    text = [w for w in text if not w in stop_words]\n",
    "    \n",
    "#    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Cleaning & Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish', ignore_stopwords = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub('\\w+', lambda x:'' if x.group().startswith('__') else x.group(), text)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAll(text):\n",
    "    text = re.sub( hash_regex, hash_repl, text )\n",
    "    text = re.sub( user_regex, user_repl, text)\n",
    "    text = re.sub( url_regex, url_repl, text )\n",
    "    \n",
    "    text = text.replace('\\'','')\n",
    "    \n",
    "    text = re.sub( word_bound_regex , punctuations_repl, text )\n",
    "    text = re.sub( rpt_regex, rpt_repl, text )\n",
    "    \n",
    "    text = clean(text)\n",
    "    #text = stopwords(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['processed_tweet'] = train_data.tweetText.apply(processAll)\n",
    "amlo_tweets['processed_tweet'] = amlo_tweets.tweetText.apply(processAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sb_stem(text):\n",
    "    text = [word if(word[0:2]=='__') else word.lower() for word in text.split() if ((len(word) >= 3) or (word in ['no','si', 'sí', 'ni']))] #keep the small words (like 'no')\n",
    "    text = [stemmer.stem(w) if w[0:2]!='__' else w for w in text ]\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['stemmed_tweet'] = train_data.processed_tweet.apply(sb_stem)\n",
    "amlo_tweets['stemmed_tweet'] = amlo_tweets.processed_tweet.apply(sb_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def bigramize(tweets, n=2):\n",
    "    bigrams=[]\n",
    "    for tweet in tweets:\n",
    "        bigrams += ngrams(tweets,n=2)\n",
    "    return bigrams\n",
    "\n",
    "def trigramize(tweets, n=3):\n",
    "    trigrams=[]\n",
    "    for tweet in tweets:\n",
    "        trigrams += ngrams(tweets,n=3)\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['bigrams'] = train_data.stemmed_tweet.apply(bigramize)\n",
    "train_data['trigrams'] = train_data.stemmed_tweet.apply(trigramize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_tweets['bigrams'] = amlo_tweets.stemmed_tweet.apply(bigramize)\n",
    "amlo_tweets['trigrams'] = amlo_tweets.stemmed_tweet.apply(trigramize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2 = train_data[['polarity_value','stemmed_tweet','bigrams','trigrams']]\n",
    "train_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data3 = train_data2.copy()\n",
    "for index, row in train_data2.iterrows():\n",
    "    for word in row.stemmed_tweet:\n",
    "        if word not in train_data3.columns:\n",
    "            train_data3[word] = 0\n",
    "        train_data3.loc[index, word] = 1\n",
    "train_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_tweets2 = amlo_tweets[['Date','stemmed_tweet','bigrams','trigrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_tweets3 = amlo_tweets2.copy()\n",
    "for index, row in amlo_tweets2.iterrows():\n",
    "    for word in row.stemmed_tweet:\n",
    "        if word not in amlo_tweets3.columns:\n",
    "            amlo_tweets3[word] = 0\n",
    "        amlo_tweets3.loc[index, word] = 1\n",
    "amlo_tweets3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data3.drop(['polarity_value','stemmed_tweet','bigrams','trigrams'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data3[['polarity_value']]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = amlo_tweets3.drop(['Date','stemmed_tweet','bigrams','trigrams'], axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = nb.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "\n",
    "print(cm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(nb, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test\n",
    "y = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, linewidth=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('spanish') + punctuation + ['rt', 'via'] + ['lopezobrador'] \n",
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stop,\n",
    "                          max_words=200,\n",
    "                          max_font_size=50, \n",
    "                          random_state=42\n",
    "                         ).generate(str(amlo_tweets['processed_tweet']))\n",
    "plt.imshow(wordcloud)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
